{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9e+cbS6AuwOmnrKUV2ZhO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrvZzZzZ/Lab1_AI_for_study/blob/main/wiki_parse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0-ck_AO6PLy",
        "outputId": "e9f89475-faeb-44e1-ca47-d987c7fd0dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (4.13.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.3)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.32.4)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2025.11.12)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-3.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.20.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=7832fb153308d18ffb53c32395a52da7e0bfb17f6861283eb7950edba760bb78\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/91/9f/00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=22d2df9a40558b4b621255102ef951e04b3d28b80f9be867015d11e2ff4ee160\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/9f/fb/364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=b014a9cb4d6aa9f3d7cb863790dbabcea58bfccd4ba060fc811b4eee67c742cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/72/f7/fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=b2bd9903654e15cb766fbc4cd2b1267faa5ea9ce16bb9be16ed7d21b819498b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.12 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-3.0.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Collecting mwclient\n",
            "  Downloading mwclient-0.11.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting mwparserfromhell\n",
            "  Downloading mwparserfromhell-0.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from mwclient) (2.0.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->mwclient) (3.3.1)\n",
            "Downloading mwclient-0.11.0-py3-none-any.whl (33 kB)\n",
            "Downloading mwparserfromhell-0.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.5/256.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=a8ae24d79bd5e6258021e0e9962955eaeca194495ec0d882207e2b5811f92215\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/3c/79/b36253689d838af4a0539782853ac3cc38a83a6591ad570dde\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: mwparserfromhell, wikipedia-api, mwclient\n",
            "Successfully installed mwclient-0.11.0 mwparserfromhell-0.7.2 wikipedia-api-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install newspaper3k\n",
        "!pip install openai mwclient mwparserfromhell tiktoken wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = key\n",
        "\n",
        "base_url = \"https://api.vsegpt.ru/v1\"\n",
        "os.environ[\"OPENAI_BASE_URL\"] = base_url"
      ],
      "metadata": {
        "id": "2SUnar0R7SpO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "LMgHhhklO-Ry"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mwparserfromhell  # Парсер для MediaWiki\n",
        "import openai  # будем использовать для токинизации\n",
        "import pandas as pd  # В DataFrame будем хранить базу знаний и результат токинизации базы знаний\n",
        "import re  # для вырезания ссылок <ref> из статей Википедии\n",
        "import tiktoken  # для подсчета токенов"
      ],
      "metadata": {
        "id": "kS9pBBvfPDr8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trafilatura requests mwparserfromhell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_e4xhTy2AN9",
        "outputId": "7bc5e484-ac70-431f-ebfb-691a7a1d5bb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trafilatura\n",
            "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: mwparserfromhell in /usr/local/lib/python3.12/dist-packages (0.7.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2025.11.12)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.4.4)\n",
            "Collecting courlan>=1.3.2 (from trafilatura)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.9.2 (from trafilatura)\n",
            "  Downloading htmldate-1.9.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura)\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (6.0.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
            "  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n",
            "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.11.3)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n",
            "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n",
            "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading htmldate-1.9.4-py3-none-any.whl (31 kB)\n",
            "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: tld, lxml_html_clean, dateparser, courlan, justext, htmldate, trafilatura\n",
            "Successfully installed courlan-1.3.2 dateparser-1.2.2 htmldate-1.9.4 justext-3.0.2 lxml_html_clean-0.4.3 tld-0.13.1 trafilatura-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import mwparserfromhell\n",
        "from typing import Set, List, Tuple\n",
        "import re\n",
        "import tiktoken\n",
        "def get_category_pages_api(\n",
        "    category_name: str,\n",
        "    base_url: str = \"https://ru.wikipedia.org\",\n",
        "    max_depth: int = 1,\n",
        "    delay: float = 0.3\n",
        ") -> Set[str]:\n",
        "    API_ENDPOINT = f\"{base_url}/w/api.php\"\n",
        "    category_name = category_name.strip()\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"MyBot/1.0 (your@email.example)\"\n",
        "    }\n",
        "\n",
        "    def recurse(cat: str, depth: int) -> Set[str]:\n",
        "        titles = set()\n",
        "        if depth < 0:\n",
        "            return titles\n",
        "\n",
        "        cmtitle = f\"Категория:{cat}\"\n",
        "\n",
        "        params = {\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"categorymembers\",\n",
        "            \"cmtitle\": cmtitle,\n",
        "            \"cmprop\": \"title|type|ns\",\n",
        "            \"cmlimit\": \"max\",\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(API_ENDPOINT, params=params, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if \"error\" in data:\n",
        "                print(f\"API error for '{cmtitle}': {data['error'].get('info', 'unknown')}\")\n",
        "                return titles\n",
        "\n",
        "            members = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
        "            if not members and \"continue\" not in data:\n",
        "                print(f\"Категория '{cmtitle}' существует, но пуста или не содержит статей на этом уровне.\")\n",
        "\n",
        "            for m in members:\n",
        "                title = m[\"title\"]\n",
        "                ns = m.get(\"ns\", 0)\n",
        "                page_type = m.get(\"type\", \"page\")\n",
        "\n",
        "                if ns == 0:\n",
        "                    titles.add(title)\n",
        "                elif page_type == \"subcat\" and depth > 0:\n",
        "                    if title.startswith(\"Категория:\"):\n",
        "                        subcat = title[len(\"Категория:\"):]\n",
        "                    else:\n",
        "                        subcat = title\n",
        "                    titles.update(recurse(subcat, depth - 1))\n",
        "\n",
        "            while \"continue\" in data:\n",
        "                params.update(data[\"continue\"])\n",
        "                response = requests.get(API_ENDPOINT, params=params, headers=headers, timeout=10)\n",
        "                data = response.json()\n",
        "                members = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
        "                for m in members:\n",
        "                    title = m[\"title\"]\n",
        "                    ns = m.get(\"ns\", 0)\n",
        "                    page_type = m.get(\"type\", \"page\")\n",
        "                    if ns == 0:\n",
        "                        titles.add(title)\n",
        "                    elif page_type == \"subcat\" and depth > 0:\n",
        "                        subcat = title[len(\"Категория:\"):] if title.startswith(\"Категория:\") else title\n",
        "                        titles.update(recurse(subcat, depth - 1))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке '{cmtitle}': {e}\")\n",
        "            return titles\n",
        "\n",
        "        time.sleep(delay)\n",
        "        return titles\n",
        "\n",
        "    return recurse(category_name, max_depth)\n",
        "\n",
        "def get_page_text(title: str, lang: str = \"ru\") -> str:\n",
        "    \"\"\"Получает wikitext статьи через action=raw.\"\"\"\n",
        "    url = f\"https://{lang}.wikipedia.org/w/index.php\"\n",
        "    params = {\n",
        "        \"action\": \"raw\",\n",
        "        \"title\": title,\n",
        "        \"textonly\": \"1\"\n",
        "    }\n",
        "    headers = {\"User-Agent\": \"MyBot/1.0 (your@email.example)\"}\n",
        "    response = requests.get(url, params=params, headers=headers, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def all_subsections_from_section(\n",
        "    section: mwparserfromhell.wikicode.Wikicode,\n",
        "    parent_titles: List[str],\n",
        "    sections_to_ignore: Set[str]\n",
        ") -> List[Tuple[List[str], str]]:\n",
        "    headings = section.filter_headings()\n",
        "    if not headings:\n",
        "        return []\n",
        "\n",
        "    first_heading = headings[0]\n",
        "    title_clean = first_heading.title.strip_code().strip()\n",
        "    title_raw = str(first_heading)\n",
        "\n",
        "    if title_clean in sections_to_ignore:\n",
        "        return []\n",
        "\n",
        "    titles = parent_titles + [title_raw]\n",
        "\n",
        "    section_nodes = section.nodes\n",
        "    content_nodes = []\n",
        "    found_heading = False\n",
        "    for node in section_nodes:\n",
        "        if isinstance(node, mwparserfromhell.nodes.Heading):\n",
        "            if not found_heading:\n",
        "                found_heading = True\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "        if found_heading:\n",
        "            content_nodes.append(node)\n",
        "\n",
        "    section_text = ''.join(str(n) for n in content_nodes).strip()\n",
        "\n",
        "    current_level = first_heading.level\n",
        "    results = [(titles, section_text)]\n",
        "\n",
        "    for subsection in section.get_sections(levels=[current_level + 1]):\n",
        "        results.extend(\n",
        "            all_subsections_from_section(subsection, titles, sections_to_ignore)\n",
        "        )\n",
        "    return results\n",
        "\n",
        "def all_subsections_from_title(\n",
        "    title: str,\n",
        "    sections_to_ignore: Set[str] = None,\n",
        "    lang: str = \"ru\"\n",
        ") -> List[Tuple[List[str], str]]:\n",
        "    if sections_to_ignore is None:\n",
        "        sections_to_ignore = {\n",
        "            \"См. также\", \"Примечания\", \"Литература\", \"Ссылки\", \"Внешние ссылки\",\n",
        "            \"See also\", \"References\", \"Bibliography\", \"External links\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        wikitext = get_page_text(title, lang=lang)\n",
        "    except Exception as e:\n",
        "        print(f\"Не удалось загрузить «{title}»: {e}\")\n",
        "        return []\n",
        "\n",
        "    parsed = mwparserfromhell.parse(wikitext)\n",
        "    headings = parsed.filter_headings()\n",
        "\n",
        "    if headings:\n",
        "        first_heading_str = str(headings[0])\n",
        "        full_text = str(parsed)\n",
        "        summary = full_text.split(first_heading_str, 1)[0].strip()\n",
        "    else:\n",
        "        summary = str(parsed).strip()\n",
        "\n",
        "    results = [([title], summary)]\n",
        "\n",
        "    for section in parsed.get_sections(levels=[2]):\n",
        "        results.extend(\n",
        "            all_subsections_from_section(section, [title], sections_to_ignore)\n",
        "        )\n",
        "    return results\n",
        "\n",
        "def clean_section(section: Tuple[List[str], str]) -> Tuple[List[str], str]:\n",
        "    titles, text = section\n",
        "    text = re.sub(r\"<ref\\b[^>]*/?>\", \"\", text)\n",
        "    text = re.sub(r\"</ref\\s*>\", \"\", text)\n",
        "    text = re.sub(r\"\\{\\{(?:[SsEe]fn|note[Tt]ag|refn?|уточнить|источник|fact)\\b[^}]*\\}\\}\", \"\", text, flags=re.IGNORECASE)\n",
        "    text = text.strip()\n",
        "    return (titles, text)\n",
        "\n",
        "def keep_section(section: Tuple[List[str], str]) -> bool:\n",
        "    _, text = section\n",
        "    words = text.split()\n",
        "    return len(words) >= 3 and len(text) >= 20"
      ],
      "metadata": {
        "id": "Jlt6_rcT5ZEa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_articles = [\n",
        "    \"Великая Отечественная война\",\n",
        "    \"Московская битва\",\n",
        "    \"Сталинградская битва\",\n",
        "    \"Курская битва\",\n",
        "    \"Операция «Багратион»\",\n",
        "    \"Блокада Ленинграда\",\n",
        "    \"Операция «Уран»\",\n",
        "    \"Операция «Кутузов»\",\n",
        "    \"Операция «Румянцев»\",\n",
        "    \"Ясско-Кишинёвская операция\",\n",
        "    \"Начало Великой Отечественной войны\",\n",
        "    \"Парад Победы\",\n",
        "    \"День Победы\",\n",
        "    \"Международная конференция в Ялте\",\n",
        "    \"Потсдамская конференция\",\n",
        "    \"Сталин, Иосиф Виссарионович\",\n",
        "    \"Жуков, Георгий Константинович\",\n",
        "    \"Рокоссовский, Константин Константинович\",\n",
        "    \"Чуйков, Василий Иванович\",\n",
        "    \"Василевский, Александр Михайлович\",\n",
        "    \"Молотов, Вячеслав Михайлович\",\n",
        "    \"Дорога жизни\",\n",
        "    \"Трудовой фронт в СССР во время Великой Отечественной войны\",\n",
        "    \"Эвакуация в СССР в годы Великой Отечественной войны\",\n",
        "    \"Обращение Молотова по радио 22 июня 1941 года\",\n",
        "    \"Речь Сталина 3 июля 1941 года\",\n",
        "    \"Песни Великой Отечественной войны\",\n",
        "    \"Берггольц, Ольга Фёдоровна\",\n",
        "    \"Ленинградская филармония в годы блокады\"\n",
        "]"
      ],
      "metadata": {
        "id": "GmHpUwpX69QO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = get_category_pages_api(\n",
        "    category_name=\"Великая Отечественная война\",\n",
        "    base_url=\"https://ru.wikipedia.org\",\n",
        "    max_depth=0,\n",
        "    delay=0.8\n",
        ")\n",
        "# titles = set(key_articles)  # ← руками заданный мини-корпус\n",
        "# print(f\"Статей: {len(titles)} (ручной выбор)\")\n",
        "print(f\"Собрано {len(titles)} статей.\")\n",
        "\n",
        "wikipedia_sections: List[Tuple[List[str], str]] = []\n",
        "\n",
        "for i, title in enumerate(titles, 1):\n",
        "    try:\n",
        "        sections = all_subsections_from_title(title, lang=\"ru\")\n",
        "        wikipedia_sections.extend(sections)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обработке «{title}»: {e}\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(f\"\\nВсего извлечено {len(wikipedia_sections)} секций из {len(titles)} статей.\")\n",
        "\n",
        "original_num = len(wikipedia_sections)\n",
        "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
        "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
        "\n",
        "print(f\"Отфильтровано {original_num - len(wikipedia_sections)} секций, осталось {len(wikipedia_sections)}.\")\n",
        "\n",
        "print(\"\\nПримеры оставшихся секций:\")\n",
        "for i, (path, text) in enumerate(wikipedia_sections[:5], 1):\n",
        "    print(f\"{i}. {' → '.join(path[-2:])}\")\n",
        "    print(f\"   {text[:100].replace(chr(10), ' ').strip()}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78UV6P4iZKhQ",
        "outputId": "f7ef7f3a-147d-4557-f915-d3b767769576"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Собрано 94 статей.\n",
            "\n",
            "Всего извлечено 582 секций из 94 статей.\n",
            "Отфильтровано 26 секций, осталось 556.\n",
            "\n",
            "Примеры оставшихся секций:\n",
            "1. Волжская рокада\n",
            "   {{Железнодорожная линия}}  '''Во́лжская рока́да''' — [[Рокадная железная дорога|рокадная линия желез...\n",
            "2. Волжская рокада → == Решение Госкомитета обороны ==\n",
            "   Осенью 1941 года [[Оккупация территории СССР войсками Третьего рейха и его союзников|наступающие час...\n",
            "3. Волжская рокада → == Изыскания и определение трассы ==\n",
            "   Генерал-майор Фёдор Алексеевич Гвоздёвский немедленно выехал из Сталинграда по берегу Волги в сторон...\n",
            "4. Волжская рокада → == Строительство ==\n",
            "   17 марта 1942 года ГКО утвердил проект Волжской рокады.  А тем временем на трассе уже были проведены...\n",
            "5. Волжская рокада → == Ввод в строй ==\n",
            "   7 августа 1942 года, на 103-й день после начала земляных работ, первый поезд прошёл по участку «Илов...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Примеры собранных статей (первые 20):\")\n",
        "for t in sorted(titles):\n",
        "    print(f\"• {t}\")\n",
        "\n",
        "print(f\"\\nВсего: {len(titles)}\")\n",
        "print(\"\\nЕсть ли нужные нам ключевые статьи?\")\n",
        "key_articles = [\n",
        "    \"Великая Отечественная война\",\n",
        "    \"Московская битва\",\n",
        "    \"Сталинградская битва\",\n",
        "    \"Курская битва\",\n",
        "    \"Операция «Багратион»\",\n",
        "    \"Блокада Ленинграда\",\n",
        "    \"Парад Победы\",\n",
        "    \"Рокоссовский, Константин Константинович\",\n",
        "    \"Чуйков, Василий Иванович\",\n",
        "    \"Сталин, Иосиф Виссарионович\"\n",
        "]\n",
        "\n",
        "found = [a for a in key_articles if any(a.lower() in t.lower() for t in titles)]\n",
        "missing = [a for a in key_articles if not any(a.lower() in t.lower() for t in titles)]\n",
        "\n",
        "print(\"Найдены:\", found)\n",
        "print(\"Отсутствуют:\", missing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0EL-rlp59XY",
        "outputId": "8b31ff26-fff3-4b1f-d366-fdbcf2416d26"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Примеры собранных статей (первые 20):\n",
            "• Агарковское движение\n",
            "• Аксайская переправа\n",
            "• Алтайский «Артек»\n",
            "• Арктические конвои\n",
            "• Арктические конвои (СССР)\n",
            "• Бой под Зализницей\n",
            "• Братский союз военнопленных\n",
            "• Будь героем!\n",
            "• Бытовые отряды\n",
            "• Великая Отечественная война\n",
            "• Великая Отечественная война (название)\n",
            "• Военно-шефские комиссии\n",
            "• Военнопленные в СССР во время Второй мировой войны\n",
            "• Военный налог (1941—1945)\n",
            "• Волжская рокада\n",
            "• Встреча на Эльбе\n",
            "• Всё для фронта! Всё для победы!\n",
            "• Выставка образцов трофейного вооружения (Москва)\n",
            "• Выступление Молотова по радио 22 июня 1941 года\n",
            "• Выступление Сталина по радио 3 июля 1941 года\n",
            "• Город первого салюта\n",
            "• Городской комитет обороны\n",
            "• Движение двухсотников\n",
            "• Движение тысячников\n",
            "• Депортация венгров и немцев Закарпатья\n",
            "• Десять сталинских ударов\n",
            "• Железнодорожная линия Астрахань — Кизляр\n",
            "• Железнодорожная линия Сталинград — Владимировка\n",
            "• Знак ранения\n",
            "• Знамя Победы\n",
            "• Ислам в годы Великой Отечественной войны\n",
            "• Использование принудительного труда немецкого гражданского населения в СССР\n",
            "• Итальянские войска в СССР (1941—1943)\n",
            "• Керченский железнодорожный мост\n",
            "• Колонны паровозов особого резерва\n",
            "• Конно-механизированная группа\n",
            "• Крампен\n",
            "• Курская дуга\n",
            "• Лагерь для интернированных лиц № III\n",
            "• Легенда о «чистом вермахте»\n",
            "• Людиновская подпольная комсомольская группа\n",
            "• Марш пленных немцев по Москве\n",
            "• Материнство и детство в блокадном Ленинграде\n",
            "• Международный день освобождения узников нацистских концлагерей\n",
            "• Международный день памяти жертв фашизма\n",
            "• Молодая гвардия (подпольная организация)\n",
            "• Наркомовские 100 грамм\n",
            "• Народное ополчение блокадного Ленинграда\n",
            "• Наше дело правое\n",
            "• Непрочитанные письма 1941-го\n",
            "• Ново-Петергофское военно-политическое училище пограничных и внутренних войск НКВД\n",
            "• Оккупация территории СССР войсками нацистской Германии и её союзников\n",
            "• Операция «Бандура»\n",
            "• Операция «Барбаросса»\n",
            "• Операция «Березино»\n",
            "• Операция «Монастырь»\n",
            "• Парад на Красной площади 7 ноября 1941 года\n",
            "• Партизанский отряд «За Отчизну»\n",
            "• Партийная мобилизация\n",
            "• Перемещённые культурные ценности\n",
            "• Периодические издания на оккупированной территории СССР в годы Великой Отечественной войны\n",
            "• Печорский десант\n",
            "• Покушение на Франца фон Папена\n",
            "• Полевой военкомат\n",
            "• Политбойцы\n",
            "• Постановление ГКО № ГКО-3399\n",
            "• Постановление ГКО № ГКО-5311\n",
            "• Потери в Великой Отечественной войне\n",
            "• Празднование Нового года в период Великой Отечественной войны\n",
            "• Предыстория Великой Отечественной войны\n",
            "• Принудительный труд венгров в СССР\n",
            "• Проверочно-фильтрационные лагеря НКВД СССР\n",
            "• Пропаганда во время Второй мировой войны\n",
            "• Противовоздушная оборона Москвы в Великую Отечественную войну\n",
            "• Реакция Сталина на начало Великой Отечественной войны\n",
            "• Рейхскомиссариат Дон-Волга\n",
            "• Соболевская акция\n",
            "• Совет по эвакуации при СНК СССР\n",
            "• Советские евреи в борьбе с нацизмом\n",
            "• Советско-германский обмен интернированными\n",
            "• Ставка Верховного главнокомандования\n",
            "• Сталин во Второй мировой войне\n",
            "• Студия военных художников имени М. Б. Грекова\n",
            "• Сын полка\n",
            "• Театр народного ополчения\n",
            "• Тезис о превентивной войне Германии против СССР\n",
            "• Трудовая мобилизация в Среднеазиатском военном округе\n",
            "• Труженики тыла\n",
            "• Фильтрационный лагерь НКВД № 188\n",
            "• Фонд обороны\n",
            "• Центральный архив Министерства обороны Российской Федерации\n",
            "• Черкасовское движение\n",
            "• Шлиссельбурская трасса\n",
            "• Эвакуация и сохранение культурных ценностей в СССР\n",
            "\n",
            "Всего: 94\n",
            "\n",
            "Есть ли нужные нам ключевые статьи?\n",
            "Найдены: ['Великая Отечественная война']\n",
            "Отсутствуют: ['Московская битва', 'Сталинградская битва', 'Курская битва', 'Операция «Багратион»', 'Блокада Ленинграда', 'Парад Победы', 'Рокоссовский, Константин Константинович', 'Чуйков, Василий Иванович', 'Сталин, Иосиф Виссарионович']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция подсчета токенов\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Возвращает число токенов в строке.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Функция разделения строк\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Разделяет строку надвое с помощью разделителя (delimiter), пытаясь сбалансировать токены с каждой стороны.\"\"\"\n",
        "\n",
        "    # Делим строку на части по разделителю, по умолчанию \\n - перенос строки\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # разделитель не найден\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # нет необходимости искать промежуточную точку\n",
        "    else:\n",
        "        # Считаем токены\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        # Предварительное разделение по середине числа токенов\n",
        "        best_diff = halfway\n",
        "        # В цикле ищем какой из разделителей, будет ближе всего к best_diff\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        # Возвращаем левую и правую часть оптимально разделенной строки\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "# Функция обрезает строку до максимально разрешенного числа токенов\n",
        "def truncated_string(\n",
        "    string: str, # строка\n",
        "    model: str, # модель\n",
        "    max_tokens: int, # максимальное число разрешенных токенов\n",
        "    print_warning: bool = True, # флаг вывода предупреждения\n",
        ") -> str:\n",
        "    \"\"\"Обрезка строки до максимально разрешенного числа токенов.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    # Обрезаем строку и декодируем обратно\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    if print_warning and len(encoded_string) > max_tokens:\n",
        "        print(f\"Предупреждение: Строка обрезана с {len(encoded_string)} токенов до {max_tokens} токенов.\")\n",
        "    # Усеченная строка\n",
        "    return truncated_string\n",
        "\n",
        "# Функция делит секции статьи на части по максимальному числу токенов\n",
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str], # секции\n",
        "    max_tokens: int = 1000, # максимальное число токенов\n",
        "    model: str = GPT_MODEL, # модель\n",
        "    max_recursion: int = 5, # максимальное число рекурсий\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Разделяет секции на список из частей секций, в каждой части не более max_tokens.\n",
        "    Каждая часть представляет собой кортеж родительских заголовков [H1, H2, ...] и текста (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # Если длина соответствует допустимой, то вернет строку\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # если в результате рекурсия не удалось разделить строку, то просто усечем ее по числу токенов\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # иначе разделим пополам и выполним рекурсию\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]: # Пробуем использовать разделители от большего к меньшему (разрыв, абзац, точка)\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # если какая-либо половина пуста, повторяем попытку с более простым разделителем\n",
        "                continue\n",
        "            else:\n",
        "                # применим рекурсию на каждой половине\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1, # уменьшаем максимальное число рекурсий\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # иначе никакого разделения найдено не было, поэтому просто обрезаем строку (должно быть очень редко)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n"
      ],
      "metadata": {
        "id": "I4eo4ERPcGk2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 1600\n",
        "wikipedia_strings = []\n",
        "for section in wikipedia_sections:\n",
        "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(wikipedia_sections)} секций Википедии поделены на {len(wikipedia_strings)} строк.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHHJfiiwcRQH",
        "outputId": "68ce6352-d758-44cc-a5a5-8809ea0d927e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предупреждение: Строка обрезана с 1918 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1893 токенов до 1600 токенов.\n",
            "407 секций Википедии поделены на 810 строк.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wikipedia_strings[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DvNp8GDcTuO",
        "outputId": "01b476ff-2a2a-4387-eaa4-50032dcb499f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Молотов, Вячеслав Михайлович\n",
            "\n",
            "{{←|Молотов}}\n",
            "{{ФИО|Скрябин, Вячеслав Михайлович}}\n",
            "{{Государственный деятель\n",
            "| оригинал имени = Вячеслав Михайлович Скрябин\n",
            "| изображение = Вячеслав Молотов (1947).jpg\n",
            "| ширина = 250\n",
            "| описание изображения = Министр иностранных дел СССР и член Политбюро ЦК ВКП (б) В. М. Молотов\n",
            "| должность = Постоянный представитель СССР при [[Международное агентство по атомной энергии|МАГАТЭ]]\n",
            "| флаг = Flag of USSR.svg\n",
            "| флаг2 = International Atomic Energy Agency Logo.svg\n",
            "| периодначало = 1960\n",
            "| периодконец = 1963\n",
            "| предшественник = [[Замятин, Леонид Митрофанович|Леонид Замятин]]\n",
            "| преемник = [[Пономаренко, Пантелеймон Кондратьевич|Пантелеймон Пономаренко]]\n",
            "| должность_2 = [[Список послов СССР и России в Монголии|Чрезвычайный и полномочный посол СССР в Монгольской Народной Республике]]\n",
            "| флаг_2 = Flag of USSR.svg\n",
            "| флаг2_2 = Flag of the Mongolian People's Republic (1945–1992).svg\n",
            "| периодначало_2 = 31 августа 1957\n",
            "| периодконец_2 = 3 июля 1960\n",
            "| предшественник_2 = [[Писарев, Василий Ильич|Василий Писарев]]\n",
            "| преемник_2 = [[Хворостухин, Алексей Иванович|Алексей Хворостухин]]\n",
            "| должность_3 = [[Комиссия советского контроля|Министр государственного контроля СССР]]\n",
            "| флаг_3 = Coat of arms of the Soviet Union (1946–1956).svg\n",
            "| флаг2_3 = Flag of the Soviet Union.svg\n",
            "| периодначало_3 = 21 ноября 1956\n",
            "| периодконец_3 = 29 июня 1957\n",
            "| предшественник_3 = [[Жаворонков, Василий Гаврилович|Василий Жаворонков]]\n",
            "| преемник_3 = [[Енютин, Георгий Васильевич|Георгий Енютин]] как председатель [[Комиссия советского контроля|Комиссии советского контроля Совета Министров СССР]]\n",
            "| премьер_3 = [[Булганин, Николай Александрович|Николай Булганин]]\n",
            "| должность_4 = [[Министерство иностранных дел СССР|Министр иностранных дел СССР]]\n",
            "| флаг_4 = Coat of arms of the Soviet Union (1946–1956).svg\n",
            "| флаг2_4 = Flag of the Soviet Union (1924–1955).svg\n",
            "| периодначало_4 = 5 марта 1953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
      ],
      "metadata": {
        "id": "U5Bdmk3qcXx7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"text\": wikipedia_strings})\n",
        "\n",
        "df['embedding'] = df.text.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
        "\n",
        "SAVE_PATH = \"./Great World Patriotic War.csv\"\n",
        "\n",
        "df.to_csv(SAVE_PATH, index=False)"
      ],
      "metadata": {
        "id": "Nr4mLYQydTaF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0d9zRdaelIJ",
        "outputId": "0789db61-b014-4929-97a1-05db96d3893f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   text       10 non-null     object\n",
            " 1   embedding  10 non-null     object\n",
            "dtypes: object(2)\n",
            "memory usage: 292.0+ bytes\n"
          ]
        }
      ]
    }
  ]
}